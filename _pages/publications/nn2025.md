---
layout: default
permalink: /publications/nn2025/
title: "A Graph-Based Safe Reinforcement Learning Method for Multi-agent
Cooperation (NN 2025)"
excerpt: "A Graph-Based Safe Reinforcement Learning Method for Multi-agent
Cooperation (NN 2025)"
author_profile: true
---

# A Graph-Based Safe Reinforcement Learning Method for Multi-agent Cooperation

## Abstract
**Safety and Restricted Communication** are two critical challenges faced by practical Multi-Agent Systems (MAS). However, most Multi-Agent Reinforcement Learning (MARL) algorithms that rely solely on reward shaping are ineffective in ensuring safety, and their applicability is rather limited due to the fully connected communication.  To address these issues, we propose a novel framework, **Graph-based Safe MARL (GS-MARL)**, to enhance the **safety and scalability** of MARL methods. Leveraging the inherent graph structure of MAS, we design a Graph Neural Network (GNN) based on message passing to aggregate local observations and communications of varying sizes. Furthermore, we develop a **constrained joint policy optimization** method in the setting of local observation to improve safety. Simulation experiments demonstrate that GS-MARL achieves a better trade-off between optimality and safety compared to baselines, and it significantly outperforms the latest methods in scenarios with a large number of agents under limited communication. The feasibility of our method is also verified by hardware implementation with **Mecanum-wheeled vehicles**.

Index Termsâ€”Safe reinforcement learning, Graph neural networks, Constrained policy optimization, Multi-agent cooperation, Collision avoidance.

## Contributions
1. We utilize the graph neural networks (GNNs) to achieve implicit communication between agents under partial observable environments, while enhancing sampling efficiency during the training phase of multi-agent tasks.

2. We employ the constrained joint policy optimization to solve safe MARL problem, which can handle multiple constraints to ensure safety during both the training and testing phases.

3. Extensive experiments, including hardware implementation, have been conducted to demonstrate the zero-shot transferring ability and safety level of GS-MARL, and we compared the effectiveness of GS-MARL with other prevailing works.

## Visualization

Here we provide some visualization of three multi-agent tasks in our experiments using the proposed GPA-RL framework.

<div class="gif-pair" data-img-width="400px" data-gap="4rem">
  <figure>
    <img src="{{ '/assets/publication/gpa-marl2025/rew_5agt_rewards.png' | relative_url }}" alt=" 1" />
    <figcaption>Training reward of 5 agents encirclement with GPA-MARL</figcaption>
  </figure>

  <figure>
    <img src="{{ '/assets/publication/gpa-marl2025/radar_monte.png' | relative_url }}" alt="2" />
    <figcaption>Monte Carlo test performance of different algorithms and tasks</figcaption>
  </figure>

</div>

The following GIFs illustrate the encirclement, formation and navigation process of multiple agents trained using our proposed GPA-RL method in environments with multiple static and dynamic obstacles.

<div class="gif-pair">
  <figure>
    <img src="{{ '/assets/publication/gpa-marl2025/encirclement_5agts.gif' | relative_url }}" alt="encirclement 5 agents" />
    <figcaption>Encirclement of 5 agents</figcaption>
  </figure>
</div>

<div class="gif-pair">
  <figure>
    <img src="{{ '/assets/publication/gpa-marl2025/formation_6agts.gif' | relative_url }}" alt="formation 6 agents" />
    <figcaption>Formation of 6 agents</figcaption>
  </figure>
</div>

<div class="gif-pair">
  <figure>
    <img src="{{ '/assets/publication/gpa-marl2025/navigation_4agts.gif' | relative_url }}" alt="navigation 4 agents" />
    <figcaption>Navigation of 4 agents</figcaption>
  </figure>
</div>